{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db07fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf406250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPostProcessor:\n",
    "    def __init__(self, data_path='merged_steam_data.csv'):\n",
    "        \"\"\"Initialize the post processor with the data path.\"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        \n",
    "    def load_model_predictions(self, model_name, predictions, actual_values):\n",
    "        \"\"\"Load predictions from a model for comparison.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the model (e.g., 'ridge', 'knn', 'ann')\n",
    "            predictions (array-like): Model predictions\n",
    "            actual_values (array-like): Actual target values\n",
    "        \"\"\"\n",
    "        self.predictions[model_name] = {\n",
    "            'predictions': predictions,\n",
    "            'actual': actual_values\n",
    "        }\n",
    "        \n",
    "    def calculate_metrics(self, model_name):\n",
    "        \"\"\"Calculate performance metrics for a specific model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the model to calculate metrics for\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing various performance metrics\n",
    "        \"\"\"\n",
    "        if model_name not in self.predictions:\n",
    "            raise ValueError(f\"No predictions found for model: {model_name}\")\n",
    "            \n",
    "        pred = self.predictions[model_name]['predictions']\n",
    "        actual = self.predictions[model_name]['actual']\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': mean_squared_error(actual, pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(actual, pred)),\n",
    "            'mae': mean_absolute_error(actual, pred),\n",
    "            'r2': r2_score(actual, pred),\n",
    "            'mean_absolute_percentage_error': np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_prediction_vs_actual(self, model_name, save_path=None):\n",
    "        \"\"\"Create a scatter plot of predictions vs actual values.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the model to plot\n",
    "            save_path (str, optional): Path to save the plot\n",
    "        \"\"\"\n",
    "        if model_name not in self.predictions:\n",
    "            raise ValueError(f\"No predictions found for model: {model_name}\")\n",
    "            \n",
    "        pred = self.predictions[model_name]['predictions']\n",
    "        actual = self.predictions[model_name]['actual']\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(actual, pred, alpha=0.5)\n",
    "        plt.plot([min(actual), max(actual)], [min(actual), max(actual)], 'r--')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title(f'{model_name.upper()} Predictions vs Actual Values')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_error_distribution(self, model_name, save_path=None):\n",
    "        \"\"\"Plot the distribution of prediction errors.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the model to plot\n",
    "            save_path (str, optional): Path to save the plot\n",
    "        \"\"\"\n",
    "        if model_name not in self.predictions:\n",
    "            raise ValueError(f\"No predictions found for model: {model_name}\")\n",
    "            \n",
    "        pred = self.predictions[model_name]['predictions']\n",
    "        actual = self.predictions[model_name]['actual']\n",
    "        errors = actual - pred\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(errors, kde=True)\n",
    "        plt.xlabel('Prediction Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{model_name.upper()} Error Distribution')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        \n",
    "    def compare_models(self, save_path=None):\n",
    "        \"\"\"Compare performance metrics across all loaded models.\n",
    "        \n",
    "        Args:\n",
    "            save_path (str, optional): Path to save the comparison plot\n",
    "        \"\"\"\n",
    "        if not self.predictions:\n",
    "            raise ValueError(\"No model predictions loaded\")\n",
    "            \n",
    "        metrics_df = pd.DataFrame()\n",
    "        for model_name in self.predictions:\n",
    "            metrics = self.calculate_metrics(model_name)\n",
    "            metrics_df[model_name] = pd.Series(metrics)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        metrics_df.T.plot(kind='bar')\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        \n",
    "        return metrics_df\n",
    "    \n",
    "    def generate_summary_report(self, output_path='model_summary.json'):\n",
    "        \"\"\"Generate a comprehensive summary report of all models.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path to save the summary report\n",
    "        \"\"\"\n",
    "        summary = {}\n",
    "        for model_name in self.predictions:\n",
    "            metrics = self.calculate_metrics(model_name)\n",
    "            summary[model_name] = {\n",
    "                'metrics': metrics,\n",
    "                'sample_size': len(self.predictions[model_name]['actual'])\n",
    "            }\n",
    "            \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "            \n",
    "        return summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    post_processor = ModelPostProcessor()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
